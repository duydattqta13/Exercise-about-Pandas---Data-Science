import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from sklearn import linear_model

def cost(x):
    m=A.shape[0]
    return 0.5/m * np.linalg.norm(A.dot(x)-b,2)**2
def grad(x):
    m=A.shape[0]
    return 1/m * A.T.dot(A.dot(x)-b)
def check_grad(x):
    eps = 1e-4
    g = np.zeros_like(x)
    for i in range(len(x)):
        x1 = x.copy(x)
        x2 = x.copy(x)
        x1[i] += eps
        x2[i] -= eps
        g[i] = (cost(x1)-cost(x2))/(2*eps)
    g_grad = grad(x)
    if np.linalg.norm(g-g_grad) > 1e-5:
        print("WARNING: Check Gradient function!")
def gradient_descent(x_init, learning_rate, iteration):
    x_list = [x_init]
    for i in range(iteration):
        x_new = x_list[-1] - learning_rate*grad(x_list[-1])
        if np.linalg.norm(grad(x_new))/len(x_new) < 0.5:
            break
        x_list.append(x_new)

    return x_list
